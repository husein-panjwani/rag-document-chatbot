import os
import re
from flask import Flask, render_template, request, jsonify
from docx import Document
import pdfplumber
import chromadb
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads/'

if not os.path.exists(app.config['UPLOAD_FOLDER']):
    os.makedirs(app.config['UPLOAD_FOLDER'])

try:
    mistral_api_key = os.environ.get("MISTRAL_API_KEY")
    if not mistral_api_key:
        raise ValueError("MISTRAL_API_KEY environment variable not set.")
    mistral_client = MistralClient(api_key=mistral_api_key)
except Exception as e:
    print(f"Error initializing Mistral client: {e}")
    mistral_client = None

chroma_client = chromadb.Client()
collection = chroma_client.get_or_create_collection(name="document_chunks")

def extract_text_and_tables_from_pdf(filepath):
    all_page_data = []
    try:
        with pdfplumber.open(filepath) as pdf:
            for page_num, page in enumerate(pdf.pages, start=1):
                page_text = page.extract_text() or ""
                tables = page.extract_tables()

                table_texts = []
                if tables:
                    for table in tables:
                        cleaned_rows = []
                        for row in table:
                            cleaned_row = ["" if cell is None else str(cell) for cell in row]
                            cleaned_rows.append(", ".join(cleaned_row))
                        table_str = "\n".join(cleaned_rows)
                        table_texts.append(table_str)

                combined_content = page_text
                if table_texts:
                    combined_content += "\n\nExtracted Table(s):\n" + "\n\n".join(table_texts)

                all_page_data.append({
                    "page": page_num,
                    "content": combined_content.strip()
                })
    except Exception as e:
        print(f"Error extracting text/tables from PDF: {e}")
        return None
    return all_page_data

def extract_text_from_docx(filepath):
    text = ""
    try:
        doc = Document(filepath)
        for para in doc.paragraphs:
            text += para.text + "\n"
        return [{"page": 1, "content": text}]
    except Exception as e:
        print(f"Error extracting text from DOCX: {e}")
        return None

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def chunk_text_smarter(text, chunk_size=500, overlap=50):
    sentences = re.split('(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= chunk_size:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

def get_embeddings(texts):
    if not mistral_client:
        print("Mistral client not initialized.")
        return None
    try:
        print(f"--> Attempting to get embeddings for {len(texts)} text chunks.")
        if not texts:
            print("--> ERROR: The list of texts to embed is empty.")
            return []
        
        response = mistral_client.embeddings(
            model="mistral-embed",
            input=texts
        )
        return [e.embedding for e in response.data]
    except Exception as e:
        print(f"Error getting embeddings from Mistral API: {e}")
        return None

def store_embeddings_with_pages(pages, filename):
    try:
        all_chunks, all_metadatas, all_ids = [], [], []
        for p in pages:
            if not p["content"]:
                continue
            
            preprocessed_text = preprocess_text(p["content"])
            chunks = chunk_text_smarter(preprocessed_text)

            for i, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                all_metadatas.append({"source": filename, "page": p["page"]})
                all_ids.append(f"{filename}_p{p['page']}_{i}")

        if not all_chunks:
            print("No chunks to store.")
            return False

        embeddings = get_embeddings(all_chunks)
        if embeddings is None:
            return False

        collection.add(
            embeddings=embeddings,
            documents=all_chunks,
            metadatas=all_metadatas,
            ids=all_ids
        )
        print(f"Stored {len(all_chunks)} chunks from {filename} with page references.")
        return True
    except Exception as e:
        print(f"Error storing embeddings: {e}")
        return False

def get_most_relevant_chunks(query, k=5):
    try:
        query_embedding = get_embeddings([query])
        if query_embedding is None or not query_embedding:
            return []

        results = collection.query(
            query_embeddings=query_embedding,
            n_results=k
        )
        
        documents = results["documents"][0]
        metadatas = results["metadatas"][0]
        return list(zip(documents, metadatas))
    except Exception as e:
        print(f"Error during semantic search: {e}")
        return []

def generate_llm_response(query, contexts):
    if not mistral_client:
        return "Mistral client not initialized. Cannot generate response."
    try:
        context_with_pages = "\n\n".join(
            [f"[Source: {meta['source']}, Page {meta['page']}] {doc}" for doc, meta in contexts]
        )

        prompt = (
            f"You are a helpful assistant. Use the context below to answer the question. "
            f"The context provides the source document and page number for each piece of information. "
            f"If the answer is not found in the context, reply: 'I cannot find the answer in the provided document.'\n\n"
            f"--- CONTEXT ---\n{context_with_pages}\n\n"
            f"--- QUESTION ---\n{query}"
        )
        
        messages = [ChatMessage(role="user", content=prompt)]
        
        response = mistral_client.chat(
            model="mistral-large-latest",
            messages=messages
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error generating LLM response: {e}")
        return "Sorry, I am unable to generate a response at this time."

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400

    filename = file.filename
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    file.save(file_path)

    file_extension = os.path.splitext(filename)[1].lower()
    if file_extension == '.pdf':
        extracted_pages = extract_text_and_tables_from_pdf(file_path)
    elif file_extension == '.docx':
        extracted_pages = extract_text_from_docx(file_path)
    else:
        os.remove(file_path)
        return jsonify({'error': 'Unsupported file type. Please upload a .pdf or .docx'}), 400

    if not extracted_pages:
        return jsonify({'error': 'Failed to extract text from the document.'}), 500

    if not store_embeddings_with_pages(extracted_pages, filename):
        return jsonify({'error': 'Failed to create and store vector embeddings.'}), 500
    
    return jsonify({'message': f'File "{filename}" processed and ready for questions.'}), 200

@app.route('/query', methods=['POST'])
def handle_query():
    data = request.json
    user_query = data.get('query')
    if not user_query:
        return jsonify({'error': 'No query provided'}), 400

    relevant_chunks = get_most_relevant_chunks(user_query, k=3)
    if not relevant_chunks:
        return jsonify({'response': 'I could not find any relevant information for that question.'})

    llm_response = generate_llm_response(user_query, relevant_chunks)
    return jsonify({'response': llm_response})

@app.route('/clear', methods=['POST'])
def clear_document():
    try:
        for file in os.listdir(app.config['UPLOAD_FOLDER']):
            os.remove(os.path.join(app.config['UPLOAD_FOLDER'], file))

        collection.delete(ids=collection.get()['ids'])
        
        return jsonify({'message': 'Documents and embeddings cleared successfully!'}), 200
    except Exception as e:
        print(f"Error clearing documents: {e}")
        return jsonify({'error': 'Failed to clear documents.'}), 500

if __name__ == '__main__':
    app.run(debug=True)

